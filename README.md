# py-wolf-goat-cabbage-qlearning
"Cabbage, goat, wolf" river crossing puzzle, solved with q-learning ğŸºğŸ¥¦ğŸâ›µ

This class uses determistic MDP formula to compute Q (and the optimal policy).

Example usage:

    wcg_arena = WolfGoatCabbageQLearning(
        start_state=[['ğŸ¥¦', 'ğŸ', 'â›µ', 'ğŸº'], [], []],
        goal_state=[[], [], ['â›µ', 'ğŸ', 'ğŸ¥¦', 'ğŸº']],
        gamma=0.8,
        max_episodes=1000,
        epsilon_greedy=True)
    solution_steps, scores, eps_list = wcg_arena.train()

Example solution:

    *** SOLUTION ***
    ({'ğŸ', 'â›µ', 'ğŸº', 'ğŸ¥¦'}, set(), set())
    ({'ğŸº', 'ğŸ¥¦'}, {'ğŸ', 'â›µ'}, set())
    ({'ğŸº', 'ğŸ¥¦'}, set(), {'ğŸ', 'â›µ'})
    ({'ğŸº', 'ğŸ¥¦'}, {'â›µ'}, {'ğŸ'})
    ({'â›µ', 'ğŸº', 'ğŸ¥¦'}, set(), {'ğŸ'})
    ({'ğŸº'}, {'â›µ', 'ğŸ¥¦'}, {'ğŸ'})
    ({'ğŸº'}, set(), {'ğŸ', 'â›µ', 'ğŸ¥¦'})
    ({'ğŸº'}, {'ğŸ', 'â›µ'}, {'ğŸ¥¦'})
    ({'ğŸ', 'â›µ', 'ğŸº'}, set(), {'ğŸ¥¦'})
    ({'ğŸ'}, {'â›µ', 'ğŸº'}, {'ğŸ¥¦'})
    ({'ğŸ'}, set(), {'â›µ', 'ğŸº', 'ğŸ¥¦'})
    ({'ğŸ'}, {'â›µ'}, {'ğŸº', 'ğŸ¥¦'})
    ({'ğŸ', 'â›µ'}, set(), {'ğŸº', 'ğŸ¥¦'})
    (set(), {'ğŸ', 'â›µ'}, {'ğŸº', 'ğŸ¥¦'})
    (set(), set(), {'ğŸ', 'â›µ', 'ğŸº', 'ğŸ¥¦'})

Parameters:

- start_state=`[['ğŸ¥¦', 'ğŸ', 'â›µ', 'ğŸº'], [], []]`
- goal_state=`[[], [], ['â›µ', 'ğŸ', 'ğŸ¥¦', 'ğŸº']]`
- gamma=0.8
- max_episodes=50000
- epsilon_greedy=True (specifies if you want to apply an e-greedy policy to compute Q)
    - min_epsilon=0.1
    - max_epsilon=1.0

Example graph for cumulative discounted reward and epsilon value per episode:

![Reward/epsilon per episode](/images/reward_epsilon.png)

